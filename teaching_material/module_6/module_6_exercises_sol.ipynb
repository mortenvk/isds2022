{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Exercises for Session 6: Web Scraping 1\n",
    "\n",
    "In session 5 you briefly touched upon extracting data from the internet. You worked with APIs which can be used to download data from a webpage in a structured way. Sometimes the webpage do not provide an API or the data you can download via the API is limited. In that case we will need to extract the data from the webpage ourselves. \n",
    "\n",
    "In the next three sessions you will learn how to extract data from a webpage when you cannot use an API. It involves mapping through the webpage (find the right URLs) and extracting the desired data from the webpage's HTML string (HTML: the underlying language behind a webpage).\n",
    "\n",
    "*(Note: I recommend to use Chrome as your browser during the next three sessions. Lectures and exercises are solely based on Chrome.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Scraping Jobnet.dk\n",
    "\n",
    "When we want to scrape a webpage, the first thing we do is to investigate the webpage. First, we need to get an overview of the URLs of all the webpages we want to scrape. Second, we download the HTML-string from the webpages. You can learn more about this in video 6.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('Xiu-acDIm28', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.1.1:** Go to  www.jobnet.dk and investigate the page. Locate the webpage that shows the job postings. Use the `request` module to extract the HTML-string of the webpage. \n",
    ">\n",
    "> Remember to add name and email to the header of your request, so the website managers can see that you are not a malicious actor.\n",
    "\n",
    "> *Note:* The HTML-string will not make a lot of sense right now, but try to take a look at it. In the next session we will learn how to extract data from the HTML-string.\n",
    "\n",
    "> *Note:* The website is in Danish, but it should be no problem for non-Danish speaking persons to solve the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "import requests\n",
    "response = requests.get('https://job.jobnet.dk/CV/FindWork?Offset=0&SortValue=BestMatch', headers={'name':'Hjalte Fejerskov Boas','email':'hfb@econ.ku.dk'})\n",
    "#response.text\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have completed exercise 6.1.1 you have scraped your first webpage! I.e., you have retrieved the HTML-string of the webpage you wanted to extract data from. In session 7 we will learn how to get the relevant data from the HTML-string. But first we want to learn about how to go through all the webpages we want to scrape and retrieve the HTML-strings behind: `mapping`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.1.2:** Start your `mapping`: We want to figure out what URLs we need to scrape to collect job posting data. \n",
    "\n",
    "> You will see that there are 20 job postings per page, and that you can click through the pages with job postings on the bottom of the page. Figure out what the structure of the URL is, so you can click through the job posting pages by changing the URL. \n",
    "\n",
    "> Describe the structure of the URL in plain words below. What is the relevant paging parameter (the parameter you need to change to go to the next webpage) and how does it behave when you change page?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *SOLUTION*\n",
    "*The parameter **Offset** is the relevant paging parameter. It starts at 0 at page 1, and then it becomes 20 at page 2 indicating the number of job postings in each page.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.1.3:** Make a list of the URLs of the first 5 webpages with job postings.\n",
    "\n",
    "> *Hint 1:* Design a `for loop` using the `range` function that changes the paging parameter in the URL.\n",
    ">\n",
    "> *Hint 2:* How do you change the paging parameter in the URL-string? Here string formatting is your friend! Read about it [here](https://realpython.com/python-string-formatting) (I recommend that you adopt the f-strings formatting which is a relatively new and nice feature in Python). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "links = []\n",
    "for offset in range(0,5*20,20):\n",
    "    url = f'https://job.jobnet.dk/CV/FindWork?Offset={offset}&SortValue=BestMatch'\n",
    "    links.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://job.jobnet.dk/CV/FindWork?Offset=0&SortValue=BestMatch',\n",
       " 'https://job.jobnet.dk/CV/FindWork?Offset=20&SortValue=BestMatch',\n",
       " 'https://job.jobnet.dk/CV/FindWork?Offset=40&SortValue=BestMatch',\n",
       " 'https://job.jobnet.dk/CV/FindWork?Offset=60&SortValue=BestMatch',\n",
       " 'https://job.jobnet.dk/CV/FindWork?Offset=80&SortValue=BestMatch']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.1.4:** Now loop through the list and scrape the HTML-strings of all 5 webpages using the `request` module again and save the HTML-strings in a list. \n",
    "\n",
    "> - Use the `time.sleep()` function to limit the rate of your calls. This is important to avoid overloading the webpage's server. Worst case, you can be banned from the website.\n",
    "\n",
    "> - ***Extra:*** Monitor the time left to completing the loop by using `tqdm.tqdm()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "import tqdm\n",
    "import time\n",
    "list_htmls = []\n",
    "for url in tqdm.tqdm(links):\n",
    "    response = requests.get(url, headers={'name':'Hjalte Fejerskov Boas','email':'hfb@econ.ku.dk'})\n",
    "    html = response.text\n",
    "    list_htmls.append(html)\n",
    "    time.sleep(0.5) #Sleep for 0.5 seconds\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the video below (video 6.2) you will learn about logging and handling exceptions. Watch it before continuing with Ex.6.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('d9fx8m7dQmI', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.1.5:** Repeat 6.1.4, but now log your activity as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "import os\n",
    "# First define the log function to gather the log information\n",
    "def log(response,logfile,output_path=os.getcwd()):\n",
    "    # Open or create the csv file\n",
    "    if os.path.isfile(logfile): #If the log file exists, open it and allow for changes     \n",
    "        log = open(logfile,'a')\n",
    "    else: #If the log file does not exist, create it and make headers for the log variables\n",
    "        log = open(logfile,'w')\n",
    "        header = ['timestamp','status_code','length','output_file']\n",
    "        log.write(';'.join(header) + \"\\n\") #Make the headers and jump to new line\n",
    "        \n",
    "    # Gather log information\n",
    "    status_code = response.status_code #Status code from the request result\n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())) #Local time\n",
    "    length = len(response.text) #Length of the HTML-string\n",
    "    \n",
    "    # Open the log file and append the gathered log information\n",
    "    with open(logfile,'a') as log:\n",
    "        log.write(f'{timestamp};{status_code};{length};{output_path}' + \"\\n\") #Append the information and jump to new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# Then scrape the webpages\n",
    "logfile = 'log.csv'\n",
    "list_htmls = []\n",
    "for url in tqdm.tqdm(links):\n",
    "    response = requests.get(url, headers={'name':'Hjalte Fejerskov Boas','email':'hfb@econ.ku.dk'})\n",
    "    html = response.text\n",
    "    list_htmls.append(html)\n",
    "    log(response, logfile) # Add this line to begin your logging\n",
    "    time.sleep(0.5) #Sleep for 0.5 seconds\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.1.6:** It is a good idea to build a scraper that can handle exceptions (for example a link that for some reason does not exist or connection problems). Build such an exception into your scraper from 6.1.5, so you do not loose the scraped data if it crashes halfway through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "import json\n",
    "logfile = 'log2.csv'\n",
    "list_htmls = []\n",
    "for url in tqdm.tqdm(links):\n",
    "    try:\n",
    "        response = requests.get(url, headers={'name':'Hjalte Fejerskov Boas','email':'hfb@econ.ku.dk'})\n",
    "    except Exception as e:\n",
    "        print(url) #Print url\n",
    "        print(e) #Print error\n",
    "        with open(\"list_htmls\", \"w\") as l: #Save the list_htmls as a json file to retrieve at another time\n",
    "            json.dump(list_htmls, l)\n",
    "        continue #Continue to next iteration of the loop\n",
    "    html = response.text\n",
    "    list_htmls.append(html)\n",
    "    log(response, logfile) # Log iterations\n",
    "    time.sleep(0.5) #Sleep for 0.5 seconds\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Locating data through the network panel\n",
    "\n",
    "Sometimes you may be fortunate to find the request that the webpage sends to the server to retrieve the data for the webpage. In that case, we can just replicate the request to receive the data in a structured format (JSON). Then we do not need to struggle with the HTML-strings.\n",
    "\n",
    "To do this, we first need to find the request. For that purpose, the **network panel** in the Chrome Developer Tools is useful. The network panel monitors all the uploads and downloads to and from the webpage. You can read more about the network panel [here](https://developer.chrome.com/docs/devtools/network/).\n",
    "\n",
    "**Watch the video below (video 6.3) before working on the exercises.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('isUxBDzfWMg', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.2.1:** Go to the job posting page at www.jobnet.dk again. Open the network panel and choose *Fetch/XHR* type ([Read more: XMLHttpRequest](https://en.wikipedia.org/wiki/XMLHttpRequest)). If you update the page, you will see all the XHR resources the page generates. \n",
    "\n",
    "> Go through all the XHRs and find the XHR that carries the information about the different job postings. What is the name of the XHR?\n",
    ">\n",
    ">*Note: There is no smart way to do this. You just need to go through all the XHRs and inspect the information they carry.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *SOLUTION*\n",
    "*The XHR is called \"Search\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.2.2:** Use the request URL to download the JSON file consisting of the first 20 job postings. Return the request result in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "import requests\n",
    "response = requests.get('https://job.jobnet.dk/CV/FindWork/Search', headers={'name':'Hjalte Fejerskov Boas','email':'hfb@econ.ku.dk'})\n",
    "result_json = response.json()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.2.3:** The JSON file consists of three different key-value pairs. We are only interested in the pair that contains the job postings. Find the right key-value pair and convert the JSON data to a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "import pandas as pd\n",
    "jobs_first20 = pd.DataFrame(result_json['JobPositionPostings'])\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.2.4:** At this point, we have information about the first 20 job postings. Now we want the job postings of the first 5 pages, i.e. the first 100 job postings. \n",
    "\n",
    "> Use the same procedure as in **Ex. 6.1.3-4** to download the first 100 postings and save them in a dataframe.\n",
    ">\n",
    "> *Note: Remember to limit the rate of your calls, log your activity, and think about how to handle exceptions.*\n",
    "\n",
    "> *Hint: Recall the paging parameter from **Ex. 6.1.2**. You can use the same paging parameter in the new request URL to loop through the 5 pages.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "links = []\n",
    "for offset in range(0,5*20,20):\n",
    "    url = f'https://job.jobnet.dk/CV/FindWork/Search?offset={offset}'\n",
    "    links.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://job.jobnet.dk/CV/FindWork/Search?offset=0',\n",
       " 'https://job.jobnet.dk/CV/FindWork/Search?offset=20',\n",
       " 'https://job.jobnet.dk/CV/FindWork/Search?offset=40',\n",
       " 'https://job.jobnet.dk/CV/FindWork/Search?offset=60',\n",
       " 'https://job.jobnet.dk/CV/FindWork/Search?offset=80']"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "logfile = 'log3.csv'\n",
    "list_htmls = []\n",
    "jobs_first100 = pd.DataFrame()\n",
    "\n",
    "for url in tqdm.tqdm(links):\n",
    "    try:\n",
    "        response = requests.get(url, headers={'name':'Hjalte Fejerskov Boas','email':'hfb@econ.ku.dk'})\n",
    "    except Exception as e:\n",
    "        print(url) #Print url\n",
    "        print(e) #Print error\n",
    "        jobs_first100.to_csv('jobs_first100.csv') #Save the dataframe as a csv file to retrieve at another time\n",
    "        continue #Continue to next iteration of the loop\n",
    "    \n",
    "    if response.ok: #Check if the response carries any data\n",
    "        result_json = response.json() #If the response carries data, then convert it to json format\n",
    "    else: #If the response does not carry any data, then print the status_code and continue to next iteration of the loop\n",
    "        print(response.status_code)\n",
    "        continue\n",
    "    \n",
    "    result_df = pd.DataFrame(result_json['JobPositionPostings']) #Convert this iteration's json file to a dataframe\n",
    "    jobs_first100 = pd.concat([jobs_first100,result_df], axis=0, ignore_index=True) #Append to the rest of the data\n",
    "    log(response, logfile)\n",
    "    time.sleep(0.5) #Sleep for 0.5 seconds\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.2.5 (optional):** What are the top 5 occupation areas with most job postings out of the 100 postings? How many job postings do the top 5 occupation areas have each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OccupationArea\n",
       "Sundhed, omsorg og personlig pleje               31\n",
       "Salg, indkøb og markedsføring                    10\n",
       "Bygge og anlæg                                    9\n",
       "Ledelse                                           7\n",
       "Transport, post, lager- og maskinførerarbejde     6\n",
       "Name: ID, dtype: int64"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "#jobs_first100.columns\n",
    "jobs_first100.groupby(jobs_first100['OccupationArea'])['ID'].count().sort_values(ascending=False)[0:5]\n",
    "### END SOLUTION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "nav_menu": {
    "height": "328px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
